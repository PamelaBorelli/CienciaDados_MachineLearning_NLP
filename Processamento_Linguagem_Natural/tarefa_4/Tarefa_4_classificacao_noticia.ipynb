{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGqdrQYN5Kys"
      },
      "source": [
        "## Passos para utilizar o Colab\n",
        "\n",
        "### Ajustar sistema para utilizar GPU T4\n",
        "### Subir a pasta zipada para o gdrive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FZy1gAkNq8l",
        "outputId": "7348b263-e55d-4691-b691-edda46d2c1e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Montar o gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thQcW4aiVEsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7248276b-eb0a-4b5c-daf2-dde87976e52a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/document_classification_class.zip\n",
            "   creating: document_classification_class/\n",
            "  inflating: document_classification_class/test.py  \n",
            "   creating: document_classification_class/checkpoints/\n",
            "   creating: document_classification_class/data/\n",
            "  inflating: document_classification_class/data/clean_news_with_splits.csv  \n",
            "  inflating: document_classification_class/data/glove.6B.100d.txt  \n",
            "  inflating: document_classification_class/deploy.py  \n",
            "   creating: document_classification_class/libs/\n",
            "  inflating: document_classification_class/libs/model.py  \n",
            "  inflating: document_classification_class/libs/nlpclasses.py  \n",
            "  inflating: document_classification_class/libs/utils.py  \n",
            "  inflating: document_classification_class/libs/__init__.py  \n",
            "  inflating: document_classification_class/looking_at_the_dataset.py  \n",
            "   creating: document_classification_class/plots/\n",
            "  inflating: document_classification_class/supervisedTraining.py  \n",
            "  inflating: document_classification_class/testing_implementation.py  \n"
          ]
        }
      ],
      "source": [
        "# descompactar a pasta zipada\n",
        "!unzip /content/drive/MyDrive/document_classification_class.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsUJ9ysdVbCI",
        "outputId": "760c6115-12a2-4a9a-e053-ed81193144e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/document_classification_class\n"
          ]
        }
      ],
      "source": [
        "# alterar para o diret√≥rio\n",
        "%cd document_classification_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PGrw08EkOhH",
        "outputId": "b525358b-de0c-44ea-dfa1-3460506ed58e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/  \u001b[01;34mlibs\u001b[0m/                      supervisedTraining.py\n",
            "\u001b[01;34mdata\u001b[0m/         looking_at_the_dataset.py  testing_implementation.py\n",
            "deploy.py     \u001b[01;34mplots\u001b[0m/                     test.py\n"
          ]
        }
      ],
      "source": [
        "# arquivos\n",
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lAWowY6qEEG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoQSG13vrGAq"
      },
      "outputs": [],
      "source": [
        "from libs.utils import make_embedding_matrix\n",
        "from libs.utils import generate_batches\n",
        "from libs.utils import monitor_training\n",
        "from libs.nlpclasses import NewsDataset\n",
        "from libs.model import NewsClassifier\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import argparse\n",
        "\n",
        "def compute_accuracy(y_pred, y_target):\n",
        "\t# y_pred.size: (batch_size, n_classes)\n",
        "\t# y_target.size: (batch_size,)\n",
        "\n",
        "\t_, y_pred_indices = y_pred.max(dim=1)\n",
        "\tn_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
        "\treturn n_correct / len(y_pred_indices) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asnDzyZHsWpn",
        "outputId": "8ad4ccf0-5d2f-4ede-d928-f0fec9088990"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] using device cuda\n",
            "[INFO] word not in glove embeddings, initialized randomly: <MASK>\n",
            "[INFO] word not in glove embeddings, initialized randomly: <UNK>\n",
            "[INFO] word not in glove embeddings, initialized randomly: <BEGIN>\n",
            "[INFO] word not in glove embeddings, initialized randomly: <END>\n",
            "[INFO] word not in glove embeddings, initialized randomly: newsfactor\n",
            "[INFO] word not in glove embeddings, initialized randomly: thedeal\n",
            "[INFO] word not in glove embeddings, initialized randomly: washingtonpost\n",
            "[INFO] word not in glove embeddings, initialized randomly: siliconvalley\n",
            "[INFO] word not in glove embeddings, initialized randomly: maccentral\n",
            "[INFO] word not in glove embeddings, initialized randomly: techweb\n",
            "[INFO] using pre-trained embeddings. Shape:\n",
            "(3297, 100)\n"
          ]
        }
      ],
      "source": [
        "path_loss = \"/content/document_classification_class/plots/try13_loss.png\"\n",
        "path_acc = \"/content/document_classification_class/plots/try13_acc.png\"\n",
        "news_csv = \"/content/document_classification_class/data/clean_news_with_splits.csv\"\n",
        "glove_filepath = \"/content/document_classification_class/data/glove.6B.100d.txt\"\n",
        "save_dir = \"/content/document_classification_class/checkpoints/model_13.tar\"\n",
        "use_glove = True\n",
        "embedding_size = 100\n",
        "hidden_dim = 200\n",
        "batch_size = 128\n",
        "learning_rate = 0.0001\n",
        "dropout = 0.5\n",
        "num_epochs = 50\n",
        "l2_regularization = 0.001\n",
        "\n",
        "train_state = {\n",
        "\t'epoch_index': 0,\n",
        "\t'train_loss': [],\n",
        "\t'train_acc': [],\n",
        "\t'val_loss': [],\n",
        "\t'val_acc': [],\n",
        "}\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "\tdevice = torch.device(\"cpu\")\n",
        "else:\n",
        "\tdevice = torch.device(\"cuda\")\n",
        "print(\"[INFO] using device {}\".format(device))\n",
        "\n",
        "# dataset e vetorizador\n",
        "dataset = NewsDataset.load_dataset_and_make_vectorizer(news_csv)\n",
        "vectorizer = dataset.get_vectorizer()\n",
        "\n",
        "# use glove or randomly initialized embeddings\n",
        "if use_glove:\n",
        "\twords = vectorizer.title_vocab._token_to_idx.keys()\n",
        "\tembeddings = make_embedding_matrix(glove_filepath=glove_filepath, words=words)\n",
        "\tprint(\"[INFO] using pre-trained embeddings. Shape:\")\n",
        "\tprint(embeddings.shape)\n",
        "else:\n",
        "\tembeddings = None\n",
        "\tprint(\"[INFO] using randomly initialized embeddings\")\n",
        "\n",
        "classifier = NewsClassifier(embedding_size = embedding_size,\n",
        "\t\t\t\tnum_embeddings = len(vectorizer.title_vocab),\n",
        "\t\t\t\thidden_dim = hidden_dim,\n",
        "\t\t\t\tnum_classes = len(vectorizer.category_vocab),\n",
        "\t\t\t\tdropout_p = dropout,\n",
        "\t\t\t\tpretrained_embeddings = embeddings,\n",
        "\t\t\t\tpadding_idx = 0)\n",
        "\n",
        "classifier = classifier.to(device)\n",
        "\n",
        "# loss and optimizer (Adam, SGD, etc)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "#optimizer = optim.SGD(classifier.parameters(), lr=learning_rate)\n",
        "optimizer = optim.Adam(classifier.parameters(),\n",
        "                       lr=learning_rate,\n",
        "\t\t\t\t\t\t\t\t\t\t\t weight_decay=l2_regularization)\n",
        "#optimizer = optim.Adam(classifier.parameters(),\n",
        "#                       lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83Pgbd-5t0vG",
        "outputId": "6b718bf5-b99b-4ef6-b9bb-653ae6636c86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] epoch 0, train loss 0.9591195252610416, val loss 0.7158102005720138\n",
            "[INFO] train_acc 62.819169207317074, val acc 73.14732142857143\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 1, train loss 0.6839826374155719, val loss 0.6709842954363142\n",
            "[INFO] train_acc 74.72370426829268, val acc 75.07254464285714\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 2, train loss 0.6551010456935662, val loss 0.6502328004155841\n",
            "[INFO] train_acc 75.45374428353658, val acc 75.75892857142857\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 3, train loss 0.6442235819086796, val loss 0.6423164680600166\n",
            "[INFO] train_acc 75.8253144054878, val acc 75.90401785714286\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 4, train loss 0.6332947399194647, val loss 0.6408410249011857\n",
            "[INFO] train_acc 76.37076028963415, val acc 76.03794642857143\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 5, train loss 0.6255300254447431, val loss 0.6323538239513125\n",
            "[INFO] train_acc 76.62681021341463, val acc 76.31138392857143\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 6, train loss 0.6195193510048274, val loss 0.6212969584124429\n",
            "[INFO] train_acc 76.90072408536585, val acc 76.6015625\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 7, train loss 0.6119858791551939, val loss 0.6161530658602714\n",
            "[INFO] train_acc 77.03291730182927, val acc 77.24888392857143\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 8, train loss 0.6089625233648027, val loss 0.6152573238526072\n",
            "[INFO] train_acc 77.32826791158537, val acc 77.02008928571429\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 9, train loss 0.6005060932621723, val loss 0.6069268882274628\n",
            "[INFO] train_acc 77.5509717987805, val acc 77.58928571428571\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 10, train loss 0.5964122983377155, val loss 0.6024104899593762\n",
            "[INFO] train_acc 77.7760575457317, val acc 77.734375\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 11, train loss 0.5872764022702851, val loss 0.5998087753142629\n",
            "[INFO] train_acc 78.18811928353658, val acc 77.83482142857143\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 12, train loss 0.5823788558837117, val loss 0.5898952416011265\n",
            "[INFO] train_acc 78.48466082317073, val acc 78.00223214285714\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 13, train loss 0.5768631105651942, val loss 0.5837621642010552\n",
            "[INFO] train_acc 78.81693025914635, val acc 78.5546875\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 14, train loss 0.5671261067888359, val loss 0.593965497825827\n",
            "[INFO] train_acc 79.17897294207317, val acc 77.67857142857143\n",
            "[INFO] epoch 15, train loss 0.5613808973533351, val loss 0.5751679084130696\n",
            "[INFO] train_acc 79.33736661585365, val acc 78.89508928571429\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 16, train loss 0.5562172995925676, val loss 0.5686246473874365\n",
            "[INFO] train_acc 79.46836890243902, val acc 79.24665178571429\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 17, train loss 0.5467519143187418, val loss 0.5722609207034111\n",
            "[INFO] train_acc 79.9876143292683, val acc 79.21316964285714\n",
            "[INFO] epoch 18, train loss 0.5424202273077354, val loss 0.5697526887059212\n",
            "[INFO] train_acc 80.2127000762195, val acc 79.15736607142857\n",
            "[INFO] epoch 19, train loss 0.5369219958963918, val loss 0.5605225311858314\n",
            "[INFO] train_acc 80.44731326219512, val acc 79.84933035714286\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 20, train loss 0.5281183257153849, val loss 0.5500010973640851\n",
            "[INFO] train_acc 80.85103849085365, val acc 80.10602678571429\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 21, train loss 0.5192424931631583, val loss 0.5556989537818091\n",
            "[INFO] train_acc 81.09041539634147, val acc 80.05580357142857\n",
            "[INFO] epoch 22, train loss 0.5153215232691387, val loss 0.5419537857174873\n",
            "[INFO] train_acc 81.35361089939025, val acc 80.28459821428571\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 23, train loss 0.5074274533587259, val loss 0.5434475392103195\n",
            "[INFO] train_acc 81.6453887195122, val acc 80.49107142857143\n",
            "[INFO] epoch 24, train loss 0.5047574610244937, val loss 0.5404530012181827\n",
            "[INFO] train_acc 81.82998285060975, val acc 80.15625\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 25, train loss 0.4999924749135971, val loss 0.5384656188743455\n",
            "[INFO] train_acc 81.92287538109755, val acc 80.61941964285714\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 26, train loss 0.4932614670748391, val loss 0.5408406406641006\n",
            "[INFO] train_acc 82.1384336890244, val acc 80.47991071428571\n",
            "[INFO] epoch 27, train loss 0.48668870311684725, val loss 0.5355957895517349\n",
            "[INFO] train_acc 82.40043826219512, val acc 80.77008928571429\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 28, train loss 0.4844865414154966, val loss 0.5486860705273492\n",
            "[INFO] train_acc 82.5314405487805, val acc 80.22879464285714\n",
            "[INFO] epoch 29, train loss 0.4798053094617477, val loss 0.5238311514258385\n",
            "[INFO] train_acc 82.69817073170732, val acc 81.22209821428571\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 30, train loss 0.4777403197637418, val loss 0.5347366416028567\n",
            "[INFO] train_acc 82.73389862804878, val acc 80.67522321428571\n",
            "[INFO] epoch 31, train loss 0.4716368541121483, val loss 0.5186894342303277\n",
            "[INFO] train_acc 82.92325647865853, val acc 81.25\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 32, train loss 0.46853394365710455, val loss 0.5271374766315733\n",
            "[INFO] train_acc 82.98637576219512, val acc 80.91517857142857\n",
            "[INFO] epoch 33, train loss 0.4654751980177513, val loss 0.5203446186014584\n",
            "[INFO] train_acc 83.1471512957317, val acc 81.29464285714286\n",
            "[INFO] epoch 34, train loss 0.46188012610484913, val loss 0.5201996905463082\n",
            "[INFO] train_acc 83.38057355182927, val acc 81.50111607142857\n",
            "[INFO] epoch 35, train loss 0.4621973836839926, val loss 0.5134988899741854\n",
            "[INFO] train_acc 83.29482660060975, val acc 81.57366071428571\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 36, train loss 0.456918853266937, val loss 0.516097739977496\n",
            "[INFO] train_acc 83.41391958841463, val acc 81.45089285714286\n",
            "[INFO] epoch 37, train loss 0.455560129862733, val loss 0.513556871669633\n",
            "[INFO] train_acc 83.61518673780488, val acc 81.43415178571429\n",
            "[INFO] epoch 38, train loss 0.4540714502743468, val loss 0.5119982617241996\n",
            "[INFO] train_acc 83.53182164634147, val acc 81.55691964285714\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 39, train loss 0.4508370223296125, val loss 0.513561959351812\n",
            "[INFO] train_acc 83.70927019817073, val acc 81.50669642857143\n",
            "[INFO] epoch 40, train loss 0.44901889512633403, val loss 0.5172263107129506\n",
            "[INFO] train_acc 83.70927019817073, val acc 81.47321428571429\n",
            "[INFO] epoch 41, train loss 0.4451240914896494, val loss 0.5033987079347882\n",
            "[INFO] train_acc 83.8962461890244, val acc 82.02008928571429\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 42, train loss 0.44714615816568454, val loss 0.5001852050423622\n",
            "[INFO] train_acc 83.86290015243902, val acc 82.08147321428571\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 43, train loss 0.44408647838707377, val loss 0.5014453325952802\n",
            "[INFO] train_acc 83.91768292682927, val acc 82.00892857142857\n",
            "[INFO] epoch 44, train loss 0.4428085086004036, val loss 0.5027559633765902\n",
            "[INFO] train_acc 84.00581173780488, val acc 81.96428571428571\n",
            "[INFO] epoch 45, train loss 0.44183239118173356, val loss 0.5015347972512245\n",
            "[INFO] train_acc 84.00581173780488, val acc 82.04799107142857\n",
            "[INFO] epoch 46, train loss 0.43813931142411583, val loss 0.49833009179149357\n",
            "[INFO] train_acc 84.1796875, val acc 81.98102678571429\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 47, train loss 0.43788957997883965, val loss 0.497549201973847\n",
            "[INFO] train_acc 84.15586890243902, val acc 82.00892857142857\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 48, train loss 0.43703337570243495, val loss 0.4948710671492985\n",
            "[INFO] train_acc 84.22017911585365, val acc 82.48325892857143\n",
            "[INFO] best validation loss updated and checkpoint saved\n",
            "[INFO] epoch 49, train loss 0.4340550946071744, val loss 0.5008187868765422\n",
            "[INFO] train_acc 84.29282583841463, val acc 81.99776785714286\n"
          ]
        }
      ],
      "source": [
        "best_val_loss = 99999.9\n",
        "\n",
        "for epoch_index in range(num_epochs):\n",
        "\n",
        "\ttrain_state['epoch_index'] = epoch_index\n",
        "\n",
        "\tdataset.set_split('train')\n",
        "\tbatch_generator = generate_batches(dataset, batch_size=batch_size, device=device)\n",
        "\n",
        "\trunning_loss = 0.0\n",
        "\trunning_acc = 0.0\n",
        "\n",
        "\tclassifier.train()\n",
        "\n",
        "\tfor batch_index, batch_dict in enumerate(batch_generator):\n",
        "\n",
        "\t\t# zero the gradients\n",
        "\t\toptimizer.zero_grad()\n",
        "\n",
        "\t\t# forward pass\n",
        "\t\ty_pred = classifier(x_in=batch_dict['x_data'], x_lengths=batch_dict['x_length'])\n",
        "\n",
        "\t\t# compute the loss\n",
        "\t\tloss = loss_func(y_pred, batch_dict['y_target'])\n",
        "\n",
        "\t\t# O termo weight_decay define a for√ßa da regulariza√ß√£o L2\n",
        "    # Quanto maior o valor, maior a penaliza√ß√£o dos pesos\n",
        "    # Geralmente √© um valor pequeno, por exemplo, 0.001\n",
        "\n",
        "\t\t#loss += l2_regularization * sum(p.pow(2.0).sum() for p in classifier.parameters())\n",
        "\n",
        "\t\t# backpropagation\n",
        "\t\tloss.backward()\n",
        "\n",
        "\t\t# take a step with optimizer\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t\tloss_batch = loss.item()\n",
        "\n",
        "\t\trunning_loss += loss_batch\n",
        "\n",
        "\t\tacc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "\n",
        "\t\trunning_acc += acc_batch\n",
        "\n",
        "\ttrain_state['train_loss'].append(running_loss/(batch_index+1))\n",
        "\ttrain_state['train_acc'].append(running_acc/(batch_index+1))\n",
        "\n",
        "\tdataset.set_split('val')\n",
        "\tbatch_generator = generate_batches(dataset, batch_size=batch_size, device=device)\n",
        "\n",
        "\trunning_loss = 0.0\n",
        "\trunning_acc = 0.0\n",
        "\n",
        "\tclassifier.eval()\n",
        "\n",
        "\tfor batch_index, batch_dict in enumerate(batch_generator):\n",
        "\n",
        "\t\t# forward pass\n",
        "\t\ty_pred = classifier(x_in=batch_dict['x_data'], x_lengths=batch_dict['x_length'])\n",
        "\n",
        "\t\t# compute the loss\n",
        "\t\tloss = loss_func(y_pred, batch_dict['y_target'])\n",
        "\n",
        "\t\tloss_batch = loss.item()\n",
        "\n",
        "\t\trunning_loss += loss_batch\n",
        "\n",
        "\t\tacc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "\n",
        "\t\trunning_acc += acc_batch\n",
        "\n",
        "\ttrain_state['val_loss'].append(running_loss/(batch_index+1))\n",
        "\ttrain_state['val_acc'].append(running_acc/(batch_index+1))\n",
        "\n",
        "\tprint(\"[INFO] epoch {}, train loss {}, val loss {}\".format(epoch_index, train_state['train_loss'][-1],\n",
        "\t\ttrain_state['val_loss'][-1]))\n",
        "\tprint(\"[INFO] train_acc {}, val acc {}\".format(train_state['train_acc'][-1], train_state['val_acc'][-1]))\n",
        "\n",
        "\tmonitor_training(train_state, path_loss, path_acc)\n",
        "\n",
        "\tif best_val_loss > train_state['val_loss'][-1]:\n",
        "\t\tbest_val_loss = train_state['val_loss'][-1]\n",
        "\n",
        "\t\tstate = {\n",
        "\t\t\t'epoch': epoch_index,\n",
        "\t\t\t'model_state': classifier.state_dict(),\n",
        "\t\t\t'optimizer_state': optimizer.state_dict(),\n",
        "\t\t\t'metrics': train_state\n",
        "\t\t}\n",
        "\n",
        "\t\ttorch.save(state, save_dir)\n",
        "\n",
        "\t\tprint(\"[INFO] best validation loss updated and checkpoint saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teste"
      ],
      "metadata": {
        "id": "JY88ik3jHMeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !mv /content/drive/MyDrive/model_1.tar /content/document_classification_class/checkpoints/model_1.tar"
      ],
      "metadata": {
        "id": "MQ-x4s0oKcDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from libs.utils import make_embedding_matrix\n",
        "from libs.utils import generate_batches\n",
        "from libs.utils import monitor_training\n",
        "from libs.nlpclasses import NewsDataset\n",
        "from libs.model import NewsClassifier\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from argparse import Namespace\n",
        "\n",
        "import re\n",
        "\n",
        "def compute_accuracy(y_pred, y_target):\n",
        "\t_, y_pred_indices = y_pred.max(dim=1)\n",
        "\tn_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
        "\treturn n_correct / len(y_pred_indices) * 100\n",
        "\n",
        "\n",
        "# change this line to the model you want to load\n",
        "save_dir = '/content/document_classification_class/checkpoints/model_13.tar'\n",
        "\n",
        "news_csv = \"/content/document_classification_class/data/clean_news_with_splits.csv\"\n",
        "glove_filepath = \"/content/document_classification_class/data/glove.6B.100d.txt\"\n",
        "\n",
        "use_glove = True\n",
        "embedding_size = 100\n",
        "hidden_dim = 200\n",
        "batch_size = 128\n",
        "learning_rate = 0.0001\n",
        "dropout = 0.5\n",
        "num_epochs = 50\n",
        "#l2_regularization = 0.001\n",
        "\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "\tdevice = torch.device(\"cpu\")\n",
        "else:\n",
        "\tdevice = torch.device(\"cuda\")\n",
        "print(\"[INFO] using device: {}\".format(device))\n",
        "\n",
        "# dataset and vectorizer\n",
        "dataset = NewsDataset.load_dataset_and_make_vectorizer(news_csv)\n",
        "vectorizer = dataset.get_vectorizer()\n",
        "\n",
        "# use glove or randomly initialized embeddings\n",
        "if use_glove:\n",
        "\twords = vectorizer.title_vocab._token_to_idx.keys()\n",
        "\tembeddings = make_embedding_matrix(glove_filepath=glove_filepath,\n",
        "\t\twords=words)\n",
        "\tprint(\"[INFO] using pre-trained embeddings. Shape\")\n",
        "\tprint(embeddings.shape)\n",
        "else:\n",
        "\tprint(\"[INFO] using randomly initialized embeddings\")\n",
        "\tembeddings = None\n",
        "\n",
        "# model\n",
        "classifier = NewsClassifier(embedding_size = embedding_size,\n",
        "\t\t\t\tnum_embeddings = len(vectorizer.title_vocab),\n",
        "\t\t\t\thidden_dim = hidden_dim,\n",
        "\t\t\t\tnum_classes=len(vectorizer.category_vocab),\n",
        "\t\t\t\tdropout_p = dropout,\n",
        "\t\t\t\tpretrained_embeddings=embeddings,\n",
        "\t\t\t\tpadding_idx=0)\n",
        "\n",
        "classifier = classifier.to(device)\n",
        "\n",
        "# loss and optimizer\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"[INFO] loading model...\")\n",
        "state = torch.load(save_dir)\n",
        "classifier.load_state_dict(state[\"model_state\"])\n",
        "\n",
        "# iterate over test dataset\n",
        "dataset.set_split('test')\n",
        "\n",
        "batch_generator = generate_batches(dataset,\n",
        "\tbatch_size=batch_size, device=device)\n",
        "\n",
        "running_loss = 0.0\n",
        "running_acc = 0.0\n",
        "\n",
        "classifier.eval()\n",
        "\n",
        "for batch_index, batch_dict in enumerate(batch_generator):\n",
        "\n",
        "\t# compute the output\n",
        "\ty_pred = classifier(x_in=batch_dict['x_data'],\n",
        "\t\tx_lengths=batch_dict['x_length'])\n",
        "\n",
        "\t# compute the loss\n",
        "\tloss = loss_func(y_pred, batch_dict['y_target'])\n",
        "\n",
        "\tloss_batch = loss.item()\n",
        "\n",
        "\trunning_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
        "\n",
        "\t# compute accuracy\n",
        "\tacc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "\trunning_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
        "\n",
        "print(\"Test loss: {}\".format(running_loss))\n",
        "print(\"Test Accuracy: {}\".format(running_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fNqK4icHJjw",
        "outputId": "2a81b7bc-f6ea-424b-8e9a-b4ab929e4c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] using device: cuda\n",
            "[INFO] word not in glove embeddings, initialized randomly: <MASK>\n",
            "[INFO] word not in glove embeddings, initialized randomly: <UNK>\n",
            "[INFO] word not in glove embeddings, initialized randomly: <BEGIN>\n",
            "[INFO] word not in glove embeddings, initialized randomly: <END>\n",
            "[INFO] word not in glove embeddings, initialized randomly: newsfactor\n",
            "[INFO] word not in glove embeddings, initialized randomly: thedeal\n",
            "[INFO] word not in glove embeddings, initialized randomly: washingtonpost\n",
            "[INFO] word not in glove embeddings, initialized randomly: siliconvalley\n",
            "[INFO] word not in glove embeddings, initialized randomly: maccentral\n",
            "[INFO] word not in glove embeddings, initialized randomly: techweb\n",
            "[INFO] using pre-trained embeddings. Shape\n",
            "(3297, 100)\n",
            "[INFO] loading model...\n",
            "Test loss: 0.48753383862120775\n",
            "Test Accuracy: 82.26562500000001\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}